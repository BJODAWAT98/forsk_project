# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QouWOBgmDqoVNOnXclJ-BVZ_a6i9rRUo
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.getcwd()

os.chdir('/content/drive/My Drive/forsk deeplearning')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data=pd.read_csv('mbti_1.csv')

data["list_messages"]=data["posts"].apply(lambda x: x.strip().split("|||"))

data.head()



import re
import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
list_stopwords=list(stopwords.words('english'))+[word.lower() for word in list(data.type.unique())]
nltk.download ('wordnet')
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer

def remove_url(message_list):
  res=""
  for message in message_list:
    if "http" in message:
      l=re.findall(r"\b(?:https?://)?(?:(?i:[1-9a-z]+\.))[^\s,]+\b",message)
      for link in l:
        message=message.replace(link,"")
      res=res+" "+message.strip()
    else:
      res=res+message.strip()
   
 
  res=re.sub('[^a-zA-Z\s]'," ",res)
  res=res.lower()
  res=res.split()
  res = [word for word in res if not word in list_stopwords]
  wl = PorterStemmer()
  res = [wl.stem(word) for word in res]
  res=" ".join(res)  
  return(res)

data["paragraph"]=data.list_messages.apply(remove_url)



map1 = {"I": 0, "E": 1}
map2 = {"N": 0, "S": 1}
map3 = {"T": 0, "F": 1}
map4 = {"J": 0, "P": 1}
data['I-E'] = data['type'].astype(str).str[0]
data['I-E'] = data['I-E'].map(map1)
data['N-S'] = data['type'].astype(str).str[1]
data['N-S'] = data['N-S'].map(map2)
data['T-F'] = data['type'].astype(str).str[2]
data['T-F'] = data['T-F'].map(map3)
data['J-P'] = data['type'].astype(str).str[3]
data['J-P'] = data['J-P'].map(map4)

import string
def text_process(mess):
    nopunc = [char for char in mess if char not in string.punctuation]
    nopunc = ''.join(nopunc)
    
    return [word for word in nopunc.split()]

data['cbd']=data.paragraph

from sklearn.feature_extraction.text import CountVectorizer

bow_transformer = CountVectorizer(analyzer="word",max_features=1500).fit(data['cbd'])

print(len(bow_transformer.vocabulary_))

messages_bow = bow_transformer.transform(data["paragraph"])

type(messages_bow)

print('Shape of Sparse Matrix: ', messages_bow.shape)
print('Amount of Non-Zero occurences: ', messages_bow.nnz)

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer().fit(messages_bow)

messages_tfidf = tfidf_transformer.transform(messages_bow).toarray()

print(len(messages_tfidf[0]))

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data['la'] = le.fit_transform(data['type'])

data.head()

"".join(data.list_messages[0])

labels_ie =data.iloc[:,3].values
labels_ns =data.iloc[:,4].values
labels_tf =data.iloc[:,5].values
labels_jp =data.iloc[:,6].values
labels=data.type.values



from sklearn.model_selection import train_test_split
features_train_ie, features_test_ie, labels_train_ie, labels_test_ie = train_test_split(messages_tfidf, labels_ie, test_size = 0.33, random_state = 7)

from sklearn.model_selection import train_test_split
features_train_ns, features_test_ns, labels_train_ns, labels_test_ns = train_test_split(messages_tfidf, labels_ns, test_size = 0.33, random_state = 7)

from sklearn.model_selection import train_test_split
features_train_tf, features_test_tf, labels_train_tf, labels_test_tf = train_test_split(messages_tfidf, labels_tf, test_size = 0.33, random_state = 7)

from sklearn.model_selection import train_test_split
features_train_jp, features_test_jp, labels_train_jp, labels_test_jp = train_test_split(messages_tfidf, labels_jp, test_size = 0.33, random_state = 7)

from sklearn.model_selection import train_test_split
features_train, features_test, labels_train, labels_test = train_test_split(messages_tfidf, labels, test_size = 0.33, random_state = 7)

print(features_train.shape)
print(labels_train.shape)

from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=2)
features_train, labels_train = sm.fit_sample(features_train, labels_train.ravel())

"""Best: 0.354439 using {'batch_size': 20, 'epochs': 100}
0.207330 (0.032416) with: {'batch_size': 10, 'epochs': 10}
0.267206 (0.028298) with: {'batch_size': 10, 'epochs': 50}
0.299209 (0.063069) with: {'batch_size': 10, 'epochs': 100}
0.274260 (0.028318) with: {'batch_size': 20, 'epochs': 10}
0.320716 (0.023497) with: {'batch_size': 20, 'epochs': 50}
0.354439 (0.026660) with: {'batch_size': 20, 'epochs': 100}
0.220234 (0.056856) with: {'batch_size': 40, 'epochs': 10}
0.303166 (0.041684) with: {'batch_size': 40, 'epochs': 50}
0.295767 (0.036351) with: {'batch_size': 40, 'epochs': 100}
0.252065 (0.035863) with: {'batch_size': 60, 'epochs': 10}
0.278906 (0.038507) with: {'batch_size': 60, 'epochs': 50}
0.266518 (0.088261) with: {'batch_size': 60, 'epochs': 100}
0.293014 (0.010425) with: {'batch_size': 80, 'epochs': 10}
0.281315 (0.042188) with: {'batch_size': 80, 'epochs': 50}
0.286992 (0.015178) with: {'batch_size': 80, 'epochs': 100}
0.218513 (0.035049) with: {'batch_size': 100, 'epochs': 10}
0.296628 (0.046451) with: {'batch_size': 100, 'epochs': 50}
0.279594 (0.029543) with: {'batch_size': 100, 'epochs': 100}
"""

import xgboost
classifier_ie = xgboost.XGBClassifier()
classifier_ns = xgboost.XGBClassifier()
classifier_tf = xgboost.XGBClassifier()
classifier_jp = xgboost.XGBClassifier()
classifier = xgboost.XGBClassifier()

classifier_ie.fit(features_train_ie, labels_train_ie)

classifier_ns.fit(features_train_ns, labels_train_ns)

classifier_tf.fit(features_train_tf, labels_train_tf)

classifier_jp.fit(features_train_jp, labels_train_jp)



labels_pred = classifier_jp.predict(features_test_jp)

from sklearn.metrics import accuracy_score
accuracy=accuracy_score(labels_test_jp,labels_pred)

accuracy

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

parameters = {
    'C': np.linspace(1, 10, 10)
             }
lr = LogisticRegression()

clf = GridSearchCV(lr, parameters, cv=5, verbose=5, n_jobs=3)
clf.fit(features_train, labels_train.ravel())

import string
def prediction_string(pred_string):
  res=""
  if "http" in pred_string:
    l=re.findall(r"\b(?:https?://)?(?:(?i:[1-9a-z]+\.))[^\s,]+\b",pred_string)
    for link in l:
      message=pred_string.replace(link,"")
    res=res+" "+pred_string.strip()
  else:
    res=res+pred_string.strip()
  res=re.sub('[^a-zA-Z\s]'," ",res)
  res=res.lower()
  res=res.split()
  res = [word for word in res if not word in list_stopwords]
  wl = PorterStemmer()
  res = [wl.stem(word) for word in res]
  res=" ".join(res)
  nopunc = [char for char in res if char not in string.punctuation]
  nopunc = ''.join(nopunc)
  a=tfidf_transformer.transform(bow_transformer.transform([nopunc]))
  return a

a="facerol everi singl time someon say get better ye mayb got better mean alway get better reason hear peopl say get better shinji evangelion hrm big secret realli call intellig wise creativ think world stupid sub par thing said insan realli must one two never realli experienc call high high probabl good health ehh like whole point suicid end life want want end live want elimin bad enjoy live enough go well never realli impli suicid anyth wonder correl depresson suicid well yeah alreadi got answer well say agr subconsci want get better get wor actual know know pretti mess head haha like mental thank haha hmm interest yeash would cool"

print(classifier1.predict(tfidf_transformer.transform(bow_transformer.transform([prediction_string(a)])).toarray()))

predict1=tfidf_transformer.transform(bow_transformer.transform([prediction_string(a)]))

predict1.shape

import pickle


project_pickle="newpickle_all.sav"

list_objects=[bow_transformer,classifier_ie,classifier_ns,classifier_tf,classifier_jp]

pickle.dump(list_objects,open(project_pickle,"wb"))

import pickle
loded_model = pickle.load(open("newpickle.sav",'rb'))



